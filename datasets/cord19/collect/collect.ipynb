{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect cord19 data\n",
    "\n",
    "This notebook outlines preliminary findings of collecting cord19 data.\n",
    "\n",
    "Screenshots and other relevant images can be found under the `img/` directory, and can be displayed in this notebook using\n",
    "\n",
    "    ![title](img/picture.png)\n",
    "\n",
    "where applicable.\n",
    "\n",
    "## Source of the data\n",
    "\n",
    "The [Semantic Scholar](https://www.semanticscholar.org/cord19) team at the Allen Institute for AI has partnered with leading research groups to provide CORD-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection options\n",
    "\n",
    "Here we show different options for collection, where applicable. Any prototype code used for data collection is provided in `collect.py`.\n",
    "\n",
    "### Option a) \n",
    "CORD19 is released **daily** and you can download a ZIP file with all the data from [here](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html). The ZIP contains the following files:\n",
    "- `changelog`: A text file summarizing changes between this and the previous version.\n",
    "- `cord_19_embeddings.tar.gz`: A collection of precomputed SPECTER document embeddings for each CORD-19 paper\n",
    "- `document_parses.tar.gz`: A collection of JSON files that contain full text parses of a subset of CORD-19 papers\n",
    "- `metadata.csv`: Metadata for all CORD-19 papers.\n",
    "\n",
    "You can find a detailed description (and data dictionary) for each file in this [page](https://github.com/allenai/cord19#overview).\n",
    "\n",
    "**Important note**: The Semantic Scholar team recommends to _primarily use metadata.csv & augment data when needed with full text in document_parses/_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option b) [where applicable]\n",
    "\n",
    "You can also access individual files like this:  \n",
    "`https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/<date_iso_str>/<file_name>`\n",
    "\n",
    "Replace `<date_iso_str>` with the release date formatted as `YYYY-MM-DD`, and `<file_name>` with one of the below:\n",
    "\n",
    "- Paper metadata: `metadata.csv`\n",
    "- Full text JSON: `document_parses.tar.gz`\n",
    "- SPECTER embeddings: `cord_19_embeddings.tar.gz`\n",
    "\n",
    "Here, I show how to collect the most recent `metadata.csv` file from the CORD19 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collect import get_latest_cord19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mget_latest_cord19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mURL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'document_parses.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cord_19_embeddings.tar.gz'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Download the latest CORD19 files and store them in the working directory.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Desktop/nesta/data-triage/datasets/cord19/collect/collect.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_latest_cord19?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latest_cord19(files=[\"metadata.csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical considerations\n",
    "This is where we consider CPU time, financial cost, disk space requirements, and last (but not least) development time/uncertainty.\n",
    "\n",
    "### CPU time\n",
    "#### Integrated collection time\n",
    "*This is an estimate of the time required to collect the data, without batching or parallelisation.*\n",
    "\n",
    "As shown below, fetching all files takes ~15min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 29.4 s, total: 1min 50s\n",
      "Wall time: 15min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "get_latest_cord19()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can the procedure be batched? Are there any caveats to this?\n",
    "IMO, no need to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real world collection time / cost\n",
    "*Assume a maximum of 200 concurrent 8GB 2-core machines*\n",
    "\n",
    "*NB (at time of writing based on [this](https://aws.amazon.com/ec2/pricing/on-demand/)) such a machine would cost $0.0944 per hour*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disk space (GB)\n",
    "\n",
    "#### By entity type, estimate how many \"rows\" there are to collect (e.g. 100s, 1000s, etc)\n",
    "\n",
    "10000s. At the time of writing, the main file (`metadata.csv`) has 274,033 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By entity type, and based on the field types, what is the estimated disk space?\n",
    "\n",
    "Disk space at the time of writing:\n",
    "- Main file (`metadata.csv`): 380MB\n",
    "- `cord_19_embeddings.tar.gz` (ZIP): 1.77GB\n",
    "- `document_parses.tar.gz` (ZIP): 2.95GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this imply for database storage costs?\n",
    "\n",
    "Negligible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development time\n",
    "*How long do you think it will take to develop the codebase for the collection?*\n",
    "\n",
    "Semantic Scholar has done the heavy-lifting by aggregating the data from various sources. The `collect.py` gives a solid starting point on how to fetch the data dumps.\n",
    "\n",
    "*What uncertainties can you foresee?*\n",
    "\n",
    "We don't know when Semantic Scholar will stop updating the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
