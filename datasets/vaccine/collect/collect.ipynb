{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Elsevier publication data (focussing on Vaccine and Vaccine-x)\n",
    "\n",
    "This notebook outlines preliminary findings of collecting vaccine-x data.\n",
    "\n",
    "Screenshots and other relevant images can be found under the `img/` directory, and can be displayed in this notebook using\n",
    "\n",
    "\n",
    "where applicable.\n",
    "\n",
    "## Sources\n",
    "\n",
    "[Vaccine-X](https://www.journals.elsevier.com/vaccine-x) which is the open access mirror journal of Elsevier's [Vaccine](https://www.journals.elsevier.com/vaccine) journal. In general, any approach taken here will be applicable to all of Elsevier's journals or their [open access journals](https://www.elsevier.com/about/open-science/open-access/open-access-journals/mirror-journals).\n",
    "\n",
    "For example:\n",
    "![Example of an article on Vaccine: X](img/vaccine-x-preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection options\n",
    "\n",
    "### Option a) CrossRef + PlumX (abstracts + extended metadata)\n",
    "\n",
    "Unfortunately everything under `journals.elsevier.com` is javascript generated, so selenium would be required to go down this route. After some digging around, I found a semi-hidden API from PlumX which *is* open in the following format:\n",
    "\n",
    "    https://plu.mx/api/v1/artifact/doi/{{doi}}?hideUsage=true\n",
    "\n",
    "where, for example, [{{doi}} = \"10.1016/j.jvacx.2018.100001\"](https://plu.mx/api/v1/artifact/doi/10.1016/j.jvacx.2018.100001?hideUsage=true)\n",
    "\n",
    "However, this strategy requires collecting the DOIs from somewhere, which is where CrossRef comes in.\n",
    "\n",
    "Strategy: \n",
    "\n",
    "- Use CrossRef to get all DOIs for every article in Vaccine: X.\n",
    "- Use PlumX to retrieve abstracts and extended metadata, from the doi.\n",
    "\n",
    "Note, the strategy was found to work for any Elsevier article, not just the open ones. For example, this strategy will also work for the journal \"Vaccine\", as opposed to \"Vaccine: X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 127 ms, sys: 20.2 ms, total: 147 ms\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "from habanero import Crossref\n",
    "crossref = Crossref(mailto='email address', ua_string='a good identifier')\n",
    "\n",
    "# Get all Vaccine: X DOIs\n",
    "response = crossref.works(filter={\"container-title\": \"Vaccine: X\"}, limit=1000), \n",
    "if type(response) is tuple and len(response) == 1:\n",
    "    response = response[0]\n",
    "items = response['message']['items']\n",
    "cr_fields = set([k for item in items for k in item.keys()])  #<--- Summary of all field names from CrossRef\n",
    "dois = [item['DOI'] for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 ms, sys: 3.42 ms, total: 25.6 ms\n",
      "Wall time: 464 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Example of PlumX metadata + abstract\n",
    "doi = dois[0]\n",
    "r = requests.get(f\"https://plu.mx/api/v1/artifact/doi/{doi}\", params={'hideUsage':False})\n",
    "metadata = r.json()\n",
    "px_fields = set(metadata.keys()) #<--- Summary of all field names from PlumX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata   # uncomment to see response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option b) Bonus: full-text (open access only)\n",
    "\n",
    "Following from option a), the PlumX API also returns the PII (Publisher Item Identifier) which is then enough to identify Elsevier's page for this article.\n",
    "\n",
    "    https://www.sciencedirect.com/science/article/pii/{{PII}}\n",
    "    \n",
    "where, for example, [{{PII}} = \"S2590136218300019\"](http://www.sciencedirect.com/science/article/pii/S2590136218300019). Again, this is javascript generated, so the strategy is therefore to use selenium to generate the full-text in the html, which is very well structured.\n",
    "\n",
    "### Option c) Microsoft Academic Graph (will give similar data as CrossRef + PlumX)\n",
    "\n",
    "Nesta already has the tools to collect the abstract data using MAG. The only disadvantage here is that the abstract text is reconstructed from the MAG \"inverted abstract\", and so a small amount of data quality is lost in the process, compared with extracting the data from PlumX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from nesta.packages.magrxiv.collect_magrxiv import get_magrxiv_articles\n",
    "out = []\n",
    "for i, article in enumerate(get_magrxiv_articles('vaccine', MAG_API_KEY)):\n",
    "    if i == 100:\n",
    "        break\n",
    "\n",
    "# >>> CPU times: user 325 ms, sys: 26.1 ms, total: 351 ms\n",
    "# >>> Wall time: 9.7 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical considerations\n",
    "This is where we consider CPU time, financial cost, disk space requirements, and last (but not least) development time/uncertainty.\n",
    "\n",
    "### CPU time\n",
    "#### Integrated collection time\n",
    "*This is an estimate of the time required to collect the data, without batching or parallelisation.*\n",
    "\n",
    "### Option a) CrossRef + PlumX\n",
    "\n",
    "With the CrossRef + PlumX method, the collection will take less than 500ms per article.\n",
    "\n",
    "For `Vaccine: X`, the number of articles is currently very low: only 57 since April 2019 - so the data collection time would only be 30 seconds.\n",
    "\n",
    "For `Vaccine`, the number of articles there are over 25,000 pier-reviewed articles, which would take a few hours.\n",
    "\n",
    "### Option b) Full-texts with selenium\n",
    "\n",
    "Budgeting 3 seconds per article, would requisite a further 3 minutes to collect `Vaccine-X` full-texts, albeit there aren't many of them.\n",
    "\n",
    "### Option c) Microsoft Academic Graph\n",
    "\n",
    "For MAG, the collection of all Vaccine abstracts would take only 45 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can the procedure be batched? Are there any caveats to this?\n",
    "\n",
    "This is unnecessary, except in the case of Option B) if this were to be scaled up to all Open-Access journals. The main caveat here is that running selenium in docker has quite a long development cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real world collection time / cost\n",
    "*Assume a maximum of 200 concurrent 8GB 2-core machines*\n",
    "\n",
    "*NB (at time of writing based on [this](https://aws.amazon.com/ec2/pricing/on-demand/)) such a machine would cost 0.0944 dollars per hour*\n",
    "\n",
    "**Option a) and c)** Costs are effectively zero, and collection time less than a few hours.\n",
    "\n",
    "**Option b)** Around 2 dollars to process 1000 articles. Realistically, batches of 1000 per hour would be feasible, such that up to 200,000 articles could be processed in an hour at a total cost of 400 dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disk space (GB)\n",
    "\n",
    "#### By entity type, estimate how many \"rows\" there are to collect (e.g. 100s, 1000s, etc)\n",
    "\n",
    "- **Articles**: Over 25,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By entity type, and based on the field types, what is the estimated disk space?\n",
    "\n",
    "Mainly string types, total around 5kB per article. Total diskspace should a maximum of around 125MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3733"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this imply for database storage costs?\n",
    "\n",
    "Neglible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development time\n",
    "*How long do you think it will take to develop the codebase for the collection?*\n",
    "*What uncertainties can you foresee?*\n",
    "\n",
    "#### Option a) CrossRef + PlumX: 2 days for development + 1 day for collection\n",
    "\n",
    "#### Option b) Selenium: 1 week for development + 1 day for collection\n",
    "There are significant uncertainties in this, possibly adding another week to development, but also potentially changing estimates on cost and time scales.\n",
    "\n",
    "#### Option c) MAG: 0 days for development + 1 day for collection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
